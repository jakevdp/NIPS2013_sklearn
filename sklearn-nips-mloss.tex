\documentclass{article}

\usepackage[accepted]{icml2012}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[numbers,square]{natbib}
\usepackage{xspace}
\usepackage{url}

\newcommand{\sklearn}{\textit{scikit-learn}\xspace}

% Dutch name sorting hack as per http://tex.stackexchange.com/a/40750/2806
\DeclareRobustCommand{\VAN}[3]{#2}

\begin{document}

\twocolumn[
\icmltitle{Scikit-Learn: Machine Learning in the Python ecosystem}

\icmlauthor{Gilles Louppe}{University of Liège, Belgium}
\icmlauthor{Gaël Varoquaux}{Parietal, INRIA Saclay, France}

\vskip 0.3in
]

\section*{Scikit-Learn}

The \sklearn\footnote{\url{http://scikit-
learn.org}}\footnote{\url{http://mloss.org/software/view/240}}
project~\citep{pedregosa2011} is an increasingly popular machine learning
library written in Python.  It is designed to be simple and efficient, useful
to both experts and non-experts, and reusable in a variety of contexts. The
primary aim of the project is to provide a compendium of efficient
implementations of classic, well-established  machine learning algorithms.
Among other things, it includes classical supervised and unsupervised learning
algorithms, tools for model evaluation and selection, as well as tools for
data preprocessing and feature engineering. \sklearn is distributed under the
3-clause  BSD license, encouraging its free use in both commercial and academic
settings.

Started in 2007, \sklearn is developed by an international team of over a dozen
core developers, mostly researchers from various fields of science. The project
also benefits from many occasional contributors proposing small bugfixes or
improvements. Development proceeds on GitHub, which greatly facilitates this
kind of collaboration. Because of the large number of developers, emphasis is
put on keeping the project maintainable. Code must follow quality guidelines,
such as style consistency and unit-test coverage. Documentation and examples
are required for all features, and major changes must pass code review by
developers not involved in the proposed change.

All algorithms within \sklearn are offered through a simple and elegant
API~\citep{buitinck2013api} consisting of a well-defined set of methods.
This API consistency across the package makes it very usable in practice: experimenting
with different learning algorithm is as simple as substituting a class
definition. Through composition interfaces, the library also offers powerful
mechanisms to express a wide variety of learning tasks within a small amount of
easy-to-read code. Finally, through duck-typing, the consistent API leads to a
library that is easily extensible, and allows user-defined estimators to be
incorporated into the \sklearn workflow without any explicit object
inheritance.


\section*{Integration in the Python ecosystem}

The library has been designed to tie in with standard  open source tools of the
scientific Python ecosystem. In particular, \sklearn leverages
NumPy~\citep{vanderwalt2011} for efficient storage and manipulation of
multi-dimensional arrays, and SciPy~\citep{oliphant2007python} for more specialized
data structures  (e.g. sparse matrices) and implementations of lower-level
scientific algorithms. The scikit-learn API is designed to avoid the
proliferation of framework code: it is
limited and non-intrusive. As such it makes \sklearn easy to use and easy to
combine with other libraries. Together with IPython~\citep{perez2007ipython}
for interactive exploration and Matplotlib~\citep{hunter2007matplotlib} for
dynamic data visualization, NumPy and SciPy constitute a comprehensive
scientific working environment that \sklearn smoothly complements with a
host of machine learning algorithms and data analysis routines.


\section*{Demonstrations}

This presentation will illustrate the use of \sklearn as a component of the
larger scientific Python environment to solve complex data analysis tasks.
Examples will include end-to-end workflows based on powerful
and popular algorithms in the library. Among others, we will show how to use
out-of-core learning with on-the-fly feature extraction to 
tackle very large natural language processing tasks, how to
exploit an IPython cluster for distributed cross-validation, or how to build and
use random forests to explore biological data.


{\scriptsize
\bibliographystyle{abbrvnat}
\DeclareRobustCommand{\VAN}[3]{#3}
\setlength{\bibsep}{1mm}
\bibliography{sklearn-nips-mloss}
}

\end{document}
